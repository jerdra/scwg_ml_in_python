{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning - Dimensionality Reduction\n",
    "\n",
    "In this notebook we'll explore some unsupervised machine learning approaches where we don't have the data handed to us on a silver platter. Rather we will try to exploit inherent structure in our data in order to compress (dimensionality reduction) the feature space of our data\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first kind of unsupervised we'll do, and perhaps the most canonical of all feature reduction techniques, is the **principal components analysis**. As usual we'll start with a very visual low-dimensional case then move onto higher dimensional cases and discuss how to deal with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_regression(n_samples = 300, n_features = 2, n_informative = 1, effective_rank = 1,\n",
    "                     noise = 3, n_targets = 1, bias=5, random_state=123)\n",
    "\n",
    "df = pd.DataFrame(np.c_[X,y])\n",
    "colnames = ['x1','x2','x3']\n",
    "df.columns = colnames\n",
    "df['x3'] -= df['x3'].mean()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 features that we wish to compress into 2. Let's first visualize how our data looks like in 3-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see pretty clearly here 2 of our features are highly correlated. This gives us opportunity to compress our data from $3 \\to 2$ dimensions without losing too much information. This is because the more correlated features are the less information you lose by removing one of the correlated features. We'll perform compression using **Principal Components Analysis**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>sklearn.decomposition.PCA</code> module works the same way as many of the supervised learning models. The idea is that we need to *learn* the best \"ellipse\" of the data in order to discover the principal components. The math behind doing this is not suited for this tutorial but hopefully we've already built an intuition with the theory component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we might want to query our PC in order to examine how many components might be useful for our compression step. We can do this by examining the variance explained by each principal component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, much of the original data's variance is explained by a single principal component. So therefore it may make sense to just drop 2 components and keep 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the data compression, we might want to display the data back in it's original 3-dimensional space. This is achievable with <code>PCA.inverse_transform()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
